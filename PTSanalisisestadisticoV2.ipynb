{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37cf8ede-3e2f-4327-8bb5-625a1200b84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "import numpy as np\n",
    "import mpmath as mp\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib qt\n",
    "\n",
    "from matplotlib.dates import DayLocator, HourLocator, DateFormatter, drange\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def get_directory(folder):\n",
    "    files= [a for a in listdir(folder) if isfile(join(folder,a))]\n",
    "    \n",
    "    return files\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\"get directory 2.0\"\n",
    "\n",
    "def get_directory_contents(folder):\n",
    "    items = os.listdir(folder)\n",
    "    files = [item for item in items if os.path.isfile(os.path.join(folder, item))]\n",
    "    folders = [item for item in items if os.path.isdir(os.path.join(folder, item))]\n",
    "    \n",
    "    return {'files': files, 'folders': folders}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d528a5e3-d3ea-441e-8c8f-1872ab91df9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "def search_the_day(fecha):\n",
    "    # Fecha del primer día del año\n",
    "    primer_dia_del_ano = datetime(fecha.year, 1, 1)\n",
    "\n",
    "    # Calcula la diferencia entre las dos fechas\n",
    "    diferencia = fecha - primer_dia_del_ano\n",
    "\n",
    "    # Obtiene el número de días transcurridos\n",
    "    dias_transcurridos = diferencia.days+1\n",
    "    #dias_transcurridos = diferencia.days\n",
    "    \n",
    "    return dias_transcurridos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f9717-ad5f-4e19-9f50-8a82a3190e91",
   "metadata": {},
   "source": [
    "## ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de6199d-65d3-403d-b985-62fb12e5b3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ITERATION(datahistograms):\n",
    "    a1=[[0.180,0.220], [0.280,0.320], [0.380,0.420],[0.480,0.520],[0.580,0.620],[0.680,0.720],[0.780,0.820],[0.880,0.920],[0.980,1.020],[1.080,1.120],[1.180,1.220],[1.280,1.320],[1.380,1.420]]\n",
    "    \n",
    "    meanlist=[]\n",
    "    stdlist=[]\n",
    "    for i in np.arange(0,len(a1)):\n",
    "        mean,std= MeanAndStd(datahistograms,a1[i][0],a1[i][1])\n",
    "        meanlist.append(mean)\n",
    "        stdlist.append(std)\n",
    "        \n",
    "    return meanlist, stdlist, a1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a20f283-5cc3-4846-9e4c-e22346d95317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GlobalFunction(main_folder,hora1,hora2):\n",
    "    \n",
    "    list_of_files=get_directory_contents(main_folder)                 \n",
    "    folders=list_of_files['folders']\n",
    "\n",
    "    print(folders)\n",
    "\n",
    "    TOTALDATA=[]\n",
    "\n",
    "    for i in np.arange(0,len(folders)):\n",
    "    #for i in np.arange(0,4):\n",
    "        main_folder2=main_folder+'/'+str(folders[i])\n",
    "        print(main_folder2)\n",
    "        list_of_files2=get_directory_contents(main_folder2)                   \n",
    "        files=list_of_files2['files']\n",
    "\n",
    "        newlist=[]\n",
    "        for item in files:                                        #  Iteracion sobre cada elemento, elimina los archivos que no son de interes\n",
    "            # Verificar si la cadena inicia con la letra \"p\"\n",
    "            if item.startswith(\"p\"):\n",
    "                newlist.append(main_folder2+'/'+str(item))\n",
    "        #print(newlist)\n",
    "\n",
    "        #for j in np.arange(0,len(newlist)):\n",
    "\n",
    "        'Iteracion for each hour of a day'\n",
    "\n",
    "        list_of_data_per_day=[] #list with all the data per hour per day\n",
    "        list_of_first_derivative_per_day=[]\n",
    "        list_of_second_derivative_per_day=[]\n",
    "        list_of_hours=[] \n",
    "        for j in np.arange(0,len(newlist)):\n",
    "        #for j in np.arange(0,1):\n",
    "\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            'Retain the date of the measurement'\n",
    "            dato=pd.read_table(newlist[j]) \n",
    "            dato=dato.iloc[np.arange(2,3)][' PTS'][2]\n",
    "\n",
    "\n",
    "            numeros = dato.split()\n",
    "            numeros_enteros = [int(numero) for numero in numeros]\n",
    "            numeros = numeros_enteros\n",
    "            from datetime import datetime\n",
    "            ano = 2000 + numeros[0]  \n",
    "            mes = numeros[1]\n",
    "            dia = numeros[2]\n",
    "            hora = numeros[3]\n",
    "            minuto = numeros[4]\n",
    "            segundo = numeros[5]\n",
    "\n",
    "            fecha_hora = datetime(ano, mes, dia, hora, minuto, segundo)\n",
    "                        #print('fecha_hora',fecha_hora)\n",
    "                        #print('fecha_hora',fecha_hora.hour)\n",
    "            list_of_hours.append(fecha_hora)\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "\n",
    "            'Wind data'\n",
    "\n",
    "            nombrearchivo = newlist[j] # Name of the file\n",
    "            titles=['HT',      'SPD',      'DIR',    'MET_QC'  ,'RAD1',      'RAD2',      'RAD3',      'CNT1',      'CNT2',      'CNT3',      'SNR1',      'SNR2',      'SNR3', 'QC1', 'QC2', 'QC3'   ]\n",
    "            data=pd.read_table(nombrearchivo, skiprows=10 ,header=None,names=titles, sep='\\s+') #,parse_dates=['t_utc'])\n",
    "            data=data.iloc[np.arange(1,len(data)),:] #delete the first row\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "            'Eliminate nans'\n",
    "            data=data.astype(object)\n",
    "            data = data.dropna()\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "            'Drop the rest of dataframes if there is more than 1'\n",
    "\n",
    "            for k in np.arange(0,len(data)):\n",
    "                if str(data['HT'][k])=='HT':\n",
    "                    data=data.iloc[np.arange(0,k),:] \n",
    "                    break\n",
    "\n",
    "            'Eliminar los 99999'\n",
    "            for k in np.arange(0,len(data)):\n",
    "                if float(data['SPD'][k])==999999 or float(data['HT'][k])>=1.5:\n",
    "                    data.drop(k, axis=0, inplace=True)\n",
    "\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "            'moving average'\n",
    "            data2=data[['HT','SPD','DIR']]\n",
    "\n",
    "            window_size = 2  # Tamaño de la ventana de media móvil\n",
    "\n",
    "            # Aplicar filtro de media móvil\n",
    "            data['SPDsmoothed'] = data2['SPD'].rolling(window=window_size, min_periods=1).mean()\n",
    "            data['DIRsmoothed'] = data2['DIR'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "            \n",
    "            # let's see what happens with the first and second derivative CENTERED DERIVATIVE \n",
    "            \n",
    "            first_ht=[]   \n",
    "            first_derivative=[]\n",
    "            second_ht=[]  \n",
    "            second_derivative=[]\n",
    "            \n",
    "            data['HT']=data['HT'].astype(float)\n",
    "            data['SPD']=data['SPD'].astype(float)\n",
    "            \n",
    "            for k in np.arange(0,len(data['HT'])-1):\n",
    "                first_ht.append(data['HT'][k])\n",
    "                derivative= ( (data['SPD'][k+1]-data['SPD'][k])/(data['HT'][k+1]-data['HT'][k]) )\n",
    "                first_derivative.append(derivative)\n",
    "            \n",
    "            for k in np.arange(1,len(data['HT'])-1):\n",
    "                second_ht.append(data['HT'][k])\n",
    "                \n",
    "                derivative= ( (data['SPD'][k+1]-2*data['SPD'][k]+data['SPD'][k-1]) / ((data['HT'][k+1]-data['HT'][k])**2) )\n",
    "                second_derivative.append(derivative)\n",
    "            \n",
    "            \n",
    "            d = {'HT_first': first_ht, 'SPD_first': first_derivative}\n",
    "            FIRST_DERIVATIVE_TABLE = pd.DataFrame(data=d)\n",
    "            \n",
    "            dd = {'HT_second': second_ht, 'SPD_second': second_derivative}\n",
    "            SECOND_DERIVATIVE_TABLE = pd.DataFrame(data=dd)\n",
    "            \n",
    "            #print(FIRST_DERIVATIVE_TABLE)\n",
    "            #print(SECOND_DERIVATIVE_TABLE)\n",
    "            \n",
    "            \n",
    "            list_of_data_per_day.append(data)\n",
    "            list_of_first_derivative_per_day.append(FIRST_DERIVATIVE_TABLE)\n",
    "            list_of_second_derivative_per_day.append(SECOND_DERIVATIVE_TABLE)\n",
    "\n",
    "        TOTALDATA.append( [list_of_data_per_day, list_of_hours, list_of_first_derivative_per_day,list_of_second_derivative_per_day ] )\n",
    " \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    datahistograms=[] \n",
    "    datahistograms1st=[] \n",
    "    datahistograms2nd=[] \n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=hora1 and TOTALDATA[k][1][l].hour<=hora2: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms.append( [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st.append( [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd.append( [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z]) ] )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    def MeanAndStd(datahistograms,km1,km2):\n",
    "        data=[]\n",
    "        for i in np.arange(0,len(datahistograms)):\n",
    "            if km1<=datahistograms[i][0]<=km2:\n",
    "                data.append(datahistograms[i][1]) \n",
    "        mean=np.nanmean(data)\n",
    "        std=np.nanstd(data)\n",
    "\n",
    "        return  mean, std\n",
    "    \n",
    "    def ITERATION(datahistograms,datahistograms1st,datahistograms2nd):\n",
    "        a1=[[0.180,0.220], [0.280,0.320], [0.380,0.420],[0.480,0.520],[0.580,0.620],[0.680,0.720],[0.780,0.820],[0.880,0.920],[0.980,1.020],[1.080,1.120],[1.180,1.220],[1.280,1.320],[1.380,1.420]]\n",
    "\n",
    "        meanlist=[]\n",
    "        stdlist=[]\n",
    "        \n",
    "        meanlist1st=[]\n",
    "        stdlist1st=[]\n",
    "        \n",
    "        meanlist2nd=[]\n",
    "        stdlist2nd=[]\n",
    "        \n",
    "        for i in np.arange(0,len(a1)):\n",
    "            mean,std= MeanAndStd(datahistograms,a1[i][0],a1[i][1])\n",
    "            meanlist.append(mean)\n",
    "            stdlist.append(std)\n",
    "            \n",
    "        for i in np.arange(0,len(a1)-1):\n",
    "            mean,std= MeanAndStd(datahistograms1st,a1[i][0],a1[i][1])\n",
    "            meanlist1st.append(mean)\n",
    "            stdlist1st.append(std)\n",
    "            \n",
    "        for i in np.arange(1,len(a1)-1):\n",
    "            mean,std= MeanAndStd(datahistograms2nd,a1[i][0],a1[i][1])\n",
    "            meanlist2nd.append(mean)\n",
    "            stdlist2nd.append(std)    \n",
    "            \n",
    "\n",
    "        return meanlist, stdlist, a1,  meanlist1st,stdlist1st,  meanlist2nd,stdlist2nd\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    means,deviations,kilometers , means1st,deviations1st,  means2nd,deviations2nd = ITERATION(datahistograms, datahistograms1st, datahistograms2nd)\n",
    "    \n",
    "    kilometersplot=[]\n",
    "    kilometersplot2=[]\n",
    "    kilometersplot3=[]\n",
    "    for i in np.arange(0,len(kilometers)):\n",
    "        kilometersplot.append( (kilometers[i][0]+kilometers[i][1])/2 )\n",
    "    for i in np.arange(0,len(kilometers)-1):\n",
    "        kilometersplot2.append( (kilometers[i][0]+kilometers[i][1])/2 )\n",
    "    for i in np.arange(1,len(kilometers)-1):\n",
    "        kilometersplot3.append( (kilometers[i][0]+kilometers[i][1])/2 )    \n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.errorbar(kilometersplot, means, yerr=deviations, linestyle='-', marker='o', capsize=5, label='Datos con error')\n",
    "    plt.xlabel('Altitude (km)')\n",
    "    plt.ylabel('Wind Speed Mean Values (m/s) ')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.errorbar(kilometersplot2, means1st, yerr=deviations1st, linestyle='-', marker='o', capsize=5, label='Datos con error')\n",
    "    plt.xlabel('Altitude (km)')\n",
    "    plt.ylabel('Wind Speed Mean Values (m/s km) ')\n",
    "    plt.title('First derivative')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.errorbar(kilometersplot3, means2nd, yerr=deviations2nd, linestyle='-', marker='o', capsize=5, label='Datos con error')\n",
    "    plt.xlabel('Altitude (km)')\n",
    "    plt.ylabel('Wind Speed Mean Values (m/s km2) ')\n",
    "    plt.title('Second derivative')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d98b5a-3fff-47a3-b74e-303c1cf3acc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GlobalFunction2(main_folder):\n",
    "    \n",
    "    list_of_files=get_directory_contents(main_folder)                 \n",
    "    folders=list_of_files['folders']\n",
    "\n",
    "    print(folders)\n",
    "\n",
    "    TOTALDATA=[]\n",
    "\n",
    "    for i in np.arange(0,len(folders)):\n",
    "    #for i in np.arange(0,4):\n",
    "        main_folder2=main_folder+'/'+str(folders[i])\n",
    "        print(main_folder2)\n",
    "        list_of_files2=get_directory_contents(main_folder2)                   \n",
    "        files=list_of_files2['files']\n",
    "\n",
    "        newlist=[]\n",
    "        for item in files:                                        #  Iteracion sobre cada elemento, elimina los archivos que no son de interes\n",
    "            # Verificar si la cadena inicia con la letra \"p\"\n",
    "            if item.startswith(\"p\"):\n",
    "                newlist.append(main_folder2+'/'+str(item))\n",
    "        #print(newlist)\n",
    "\n",
    "        #for j in np.arange(0,len(newlist)):\n",
    "\n",
    "        'Iteracion for each hour of a day'\n",
    "\n",
    "        list_of_data_per_day=[] #list with all the data per hour per day\n",
    "        list_of_first_derivative_per_day=[]\n",
    "        list_of_second_derivative_per_day=[]\n",
    "        list_of_hours=[] \n",
    "        for j in np.arange(0,len(newlist)):\n",
    "        #for j in np.arange(0,1):\n",
    "\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            'Retain the date of the measurement'\n",
    "            dato=pd.read_table(newlist[j]) \n",
    "            dato=dato.iloc[np.arange(2,3)][' PTS'][2]\n",
    "\n",
    "\n",
    "            numeros = dato.split()\n",
    "            numeros_enteros = [int(numero) for numero in numeros]\n",
    "            numeros = numeros_enteros\n",
    "            from datetime import datetime\n",
    "            ano = 2000 + numeros[0]  \n",
    "            mes = numeros[1]\n",
    "            dia = numeros[2]\n",
    "            hora = numeros[3]\n",
    "            minuto = numeros[4]\n",
    "            segundo = numeros[5]\n",
    "\n",
    "            fecha_hora = datetime(ano, mes, dia, hora, minuto, segundo)\n",
    "                        #print('fecha_hora',fecha_hora)\n",
    "                        #print('fecha_hora',fecha_hora.hour)\n",
    "            list_of_hours.append(fecha_hora)\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "\n",
    "            'Wind data'\n",
    "\n",
    "            nombrearchivo = newlist[j] # Name of the file\n",
    "            titles=['HT',      'SPD',      'DIR',    'MET_QC'  ,'RAD1',      'RAD2',      'RAD3',      'CNT1',      'CNT2',      'CNT3',      'SNR1',      'SNR2',      'SNR3', 'QC1', 'QC2', 'QC3'   ]\n",
    "            data=pd.read_table(nombrearchivo, skiprows=10 ,header=None,names=titles, sep='\\s+') #,parse_dates=['t_utc'])\n",
    "            data=data.iloc[np.arange(1,len(data)),:] #delete the first row\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "            'Eliminate nans'\n",
    "            data=data.astype(object)\n",
    "            data = data.dropna()\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "            'Drop the rest of dataframes if there is more than 1'\n",
    "\n",
    "            for k in np.arange(0,len(data)):\n",
    "                if str(data['HT'][k])=='HT':\n",
    "                    data=data.iloc[np.arange(0,k),:] \n",
    "                    break\n",
    "\n",
    "            'Eliminar los 99999'\n",
    "            for k in np.arange(0,len(data)):\n",
    "                if float(data['SPD'][k])==999999 or float(data['HT'][k])>=1.5:\n",
    "                    data.drop(k, axis=0, inplace=True)\n",
    "\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "            'moving average'\n",
    "            data2=data[['HT','SPD','DIR']]\n",
    "\n",
    "            window_size = 2  # Tamaño de la ventana de media móvil\n",
    "\n",
    "            # Aplicar filtro de media móvil\n",
    "            data['SPDsmoothed'] = data2['SPD'].rolling(window=window_size, min_periods=1).mean()\n",
    "            data['DIRsmoothed'] = data2['DIR'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "            \n",
    "            # let's see what happens with the first and second derivative CENTERED DERIVATIVE \n",
    "            \n",
    "            first_ht=[]   \n",
    "            first_derivative=[]\n",
    "            second_ht=[]  \n",
    "            second_derivative=[]\n",
    "            \n",
    "            data['HT']=data['HT'].astype(float)\n",
    "            data['SPD']=data['SPD'].astype(float)\n",
    "            \n",
    "            for k in np.arange(0,len(data['HT'])-1):\n",
    "                first_ht.append(data['HT'][k])\n",
    "                derivative= ( (data['SPD'][k+1]-data['SPD'][k])/(data['HT'][k+1]-data['HT'][k]) )\n",
    "                first_derivative.append(derivative)\n",
    "            \n",
    "            for k in np.arange(1,len(data['HT'])-1):\n",
    "                second_ht.append(data['HT'][k])\n",
    "                \n",
    "                derivative= ( (data['SPD'][k+1]-2*data['SPD'][k]+data['SPD'][k-1]) / ((data['HT'][k+1]-data['HT'][k])**2) )\n",
    "                second_derivative.append(derivative)\n",
    "            \n",
    "            \n",
    "            d = {'HT_first': first_ht, 'SPD_first': first_derivative}\n",
    "            FIRST_DERIVATIVE_TABLE = pd.DataFrame(data=d)\n",
    "            \n",
    "            dd = {'HT_second': second_ht, 'SPD_second': second_derivative}\n",
    "            SECOND_DERIVATIVE_TABLE = pd.DataFrame(data=dd)\n",
    "            \n",
    "            #print(FIRST_DERIVATIVE_TABLE)\n",
    "            #print(SECOND_DERIVATIVE_TABLE)\n",
    "            \n",
    "            \n",
    "            list_of_data_per_day.append(data)\n",
    "            list_of_first_derivative_per_day.append(FIRST_DERIVATIVE_TABLE)\n",
    "            list_of_second_derivative_per_day.append(SECOND_DERIVATIVE_TABLE)\n",
    "\n",
    "        TOTALDATA.append( [list_of_data_per_day, list_of_hours, list_of_first_derivative_per_day,list_of_second_derivative_per_day ] )\n",
    " \n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    datahistograms1=[] \n",
    "    datahistograms1st1=[] \n",
    "    datahistograms2nd1=[] \n",
    "    \n",
    "    datahistograms2=[] \n",
    "    datahistograms1st2=[] \n",
    "    datahistograms2nd2=[] \n",
    "    \n",
    "    datahistograms3=[] \n",
    "    datahistograms1st3=[] \n",
    "    datahistograms2nd3=[] \n",
    "    \n",
    "    datahistograms4=[] \n",
    "    datahistograms1st4=[] \n",
    "    datahistograms2nd4=[] \n",
    "    \n",
    "    \n",
    "    #Iteration for 4 time intervals\n",
    "    \n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=0 and TOTALDATA[k][1][l].hour<=6: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms1.append( [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st1.append( [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd1.append( [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z]) ] )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=6 and TOTALDATA[k][1][l].hour<=12: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms2.append( [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st2.append( [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd2.append( [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z]) ] )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=12 and TOTALDATA[k][1][l].hour<=18: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms3.append( [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st3.append( [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd3.append( [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z]) ] )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=18:    #and TOTALDATA[k][1][l].hour<=hora2: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms4.append( [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st4.append( [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd4.append( [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z]) ] )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    def MeanAndStd(datahistograms,km1,km2):\n",
    "        data=[]\n",
    "        for i in np.arange(0,len(datahistograms)):\n",
    "            if km1<=datahistograms[i][0]<=km2:\n",
    "                data.append(datahistograms[i][1]) \n",
    "        mean=np.nanmean(data)\n",
    "        std=np.nanstd(data)\n",
    "\n",
    "        return  mean, std\n",
    "    \n",
    "    def ITERATION(datahistograms,datahistograms1st,datahistograms2nd):\n",
    "        a1=[[0.180,0.220], [0.280,0.320], [0.380,0.420],[0.480,0.520],[0.580,0.620],[0.680,0.720],[0.780,0.820],[0.880,0.920],[0.980,1.020],[1.080,1.120],[1.180,1.220],[1.280,1.320],[1.380,1.420]]\n",
    "\n",
    "        meanlist=[]\n",
    "        stdlist=[]\n",
    "        \n",
    "        meanlist1st=[]\n",
    "        stdlist1st=[]\n",
    "        \n",
    "        meanlist2nd=[]\n",
    "        stdlist2nd=[]\n",
    "        \n",
    "        for i in np.arange(0,len(a1)):\n",
    "            mean,std= MeanAndStd(datahistograms,a1[i][0],a1[i][1])\n",
    "            meanlist.append(mean)\n",
    "            stdlist.append(std)\n",
    "            \n",
    "        for i in np.arange(0,len(a1)-1):\n",
    "            mean,std= MeanAndStd(datahistograms1st,a1[i][0],a1[i][1])\n",
    "            meanlist1st.append(mean)\n",
    "            stdlist1st.append(std)\n",
    "            \n",
    "        for i in np.arange(1,len(a1)-1):\n",
    "            mean,std= MeanAndStd(datahistograms2nd,a1[i][0],a1[i][1])\n",
    "            meanlist2nd.append(mean)\n",
    "            stdlist2nd.append(std)    \n",
    "            \n",
    "\n",
    "        return meanlist, stdlist, a1,  meanlist1st,stdlist1st,  meanlist2nd,stdlist2nd\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    means1,deviations1,kilometers1 , means1st1,deviations1st1,  means2nd1,deviations2nd1 = ITERATION(datahistograms1, datahistograms1st1, datahistograms2nd1)\n",
    "    means2,deviations2,kilometers2 , means1st2,deviations1st2,  means2nd2,deviations2nd2 = ITERATION(datahistograms2, datahistograms1st2, datahistograms2nd2)\n",
    "    means3,deviations3,kilometers3 , means1st3,deviations1st3,  means2nd3,deviations2nd3 = ITERATION(datahistograms3, datahistograms1st3, datahistograms2nd3)\n",
    "    means4,deviations4,kilometers4 , means1st4,deviations1st4,  means2nd4,deviations2nd4 = ITERATION(datahistograms4, datahistograms1st4, datahistograms2nd4)\n",
    "    \n",
    "    \n",
    "    kilometersplot=[]\n",
    "    kilometersplot2=[]\n",
    "    kilometersplot3=[]\n",
    "    for i in np.arange(0,len(kilometers1)):\n",
    "        kilometersplot.append( (kilometers1[i][0]+kilometers1[i][1])/2 )\n",
    "    for i in np.arange(0,len(kilometers1)-1):\n",
    "        kilometersplot2.append( (kilometers1[i][0]+kilometers1[i][1])/2 )\n",
    "    for i in np.arange(1,len(kilometers1)-1):\n",
    "        kilometersplot3.append( (kilometers1[i][0]+kilometers1[i][1])/2 )    \n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.style.use('tableau-colorblind10')\n",
    "    plt.errorbar(kilometersplot, means1, yerr=deviations1, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(kilometersplot, means2, yerr=deviations2, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(kilometersplot, means3, yerr=deviations3, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(kilometersplot, means4, yerr=deviations4, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    plt.legend()\n",
    "    #plt.ylim(0,20)\n",
    "    plt.xlabel('Altura (km)',fontsize=13)\n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.errorbar(kilometersplot2, means1st1, yerr=deviations1st1, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(kilometersplot2, means1st2, yerr=deviations1st2, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(kilometersplot2, means1st3, yerr=deviations1st3, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(kilometersplot2, means1st4, yerr=deviations1st4, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    plt.legend()\n",
    "    #plt.ylim(-25,25)\n",
    "    plt.xlabel('Altura (km)', fontsize=13)\n",
    "    plt.ylabel(r'$\\nabla V$'+' (m/s km)', fontsize=13 )\n",
    "    #plt.title('First derivative')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.errorbar(kilometersplot3, means2nd1, yerr=deviations2nd1, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(kilometersplot3, means2nd2, yerr=deviations2nd2, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(kilometersplot3, means2nd3, yerr=deviations2nd3, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(kilometersplot3, means2nd4, yerr=deviations2nd4, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    plt.legend()\n",
    "    #plt.ylim(-220,220)\n",
    "    plt.xlabel('Altura (km)', fontsize=13)\n",
    "    plt.ylabel(r'$\\nabla^2 V$'+' (m/s km2)', fontsize=13 )\n",
    "    #plt.title('Second derivative')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cea6a3-5f2f-4b15-aba6-9687c9b902f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GlobalFunction3(main_folder):\n",
    "    \n",
    "    list_of_files=get_directory_contents(main_folder)                 \n",
    "    folders=list_of_files['folders']\n",
    "\n",
    "    print(folders)\n",
    "\n",
    "    TOTALDATA=[]\n",
    "\n",
    "    for i in np.arange(0,len(folders)):\n",
    "    #for i in np.arange(0,10):\n",
    "        main_folder2=main_folder+'/'+str(folders[i])\n",
    "        print(main_folder2)\n",
    "        list_of_files2=get_directory_contents(main_folder2)                   \n",
    "        files=list_of_files2['files']\n",
    "\n",
    "        newlist=[]\n",
    "        for item in files:                                        #  Iteracion sobre cada elemento, elimina los archivos que no son de interes\n",
    "            # Verificar si la cadena inicia con la letra \"p\"\n",
    "            if item.startswith(\"p\"):\n",
    "                newlist.append(main_folder2+'/'+str(item))\n",
    "        #print(newlist)\n",
    "\n",
    "        #for j in np.arange(0,len(newlist)):\n",
    "\n",
    "        'Iteracion for each hour of a day'\n",
    "\n",
    "        list_of_data_per_day=[] #list with all the data per hour per day\n",
    "        list_of_first_derivative_per_day=[]\n",
    "        list_of_second_derivative_per_day=[]\n",
    "        list_of_hours=[] \n",
    "        for j in np.arange(0,len(newlist)):\n",
    "        #for j in np.arange(0,1):\n",
    "\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            'Retain the date of the measurement'\n",
    "            dato=pd.read_table(newlist[j]) \n",
    "            dato=dato.iloc[np.arange(2,3)][' PTS'][2]\n",
    "\n",
    "\n",
    "            numeros = dato.split()\n",
    "            numeros_enteros = [int(numero) for numero in numeros]\n",
    "            numeros = numeros_enteros\n",
    "            from datetime import datetime\n",
    "            ano = 2000 + numeros[0]  \n",
    "            mes = numeros[1]\n",
    "            dia = numeros[2]\n",
    "            hora = numeros[3]\n",
    "            minuto = numeros[4]\n",
    "            segundo = numeros[5]\n",
    "\n",
    "            fecha_hora = datetime(ano, mes, dia, hora, minuto, segundo)\n",
    "                        #print('fecha_hora',fecha_hora)\n",
    "                        #print('fecha_hora',fecha_hora.hour)\n",
    "            list_of_hours.append(fecha_hora)\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "\n",
    "            'Wind data'\n",
    "\n",
    "            nombrearchivo = newlist[j] # Name of the file\n",
    "            titles=['HT',      'SPD',      'DIR',    'MET_QC'  ,'RAD1',      'RAD2',      'RAD3',      'CNT1',      'CNT2',      'CNT3',      'SNR1',      'SNR2',      'SNR3', 'QC1', 'QC2', 'QC3'   ]\n",
    "            data=pd.read_table(nombrearchivo, skiprows=10 ,header=None,names=titles, sep='\\s+') #,parse_dates=['t_utc'])\n",
    "            data=data.iloc[np.arange(1,len(data)),:] #delete the first row\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "            'Eliminate nans'\n",
    "            data=data.astype(object)\n",
    "            data = data.dropna()\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "            'Drop the rest of dataframes if there is more than 1'\n",
    "\n",
    "            for k in np.arange(0,len(data)):\n",
    "                if str(data['HT'][k])=='HT':\n",
    "                    data=data.iloc[np.arange(0,k),:] \n",
    "                    break\n",
    "\n",
    "            'Eliminar los 99999'\n",
    "            for k in np.arange(0,len(data)):\n",
    "                if float(data['SPD'][k])==999999 or float(data['HT'][k])>=1.5:\n",
    "                    data.drop(k, axis=0, inplace=True)\n",
    "\n",
    "            data=data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "            'moving average'\n",
    "            data2=data[['HT','SPD','DIR']]\n",
    "\n",
    "            window_size = 2  # Tamaño de la ventana de media móvil\n",
    "\n",
    "            # Aplicar filtro de media móvil\n",
    "            data['SPDsmoothed'] = data2['SPD'].rolling(window=window_size, min_periods=1).mean()\n",
    "            data['DIRsmoothed'] = data2['DIR'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "            \n",
    "            # let's see what happens with the first and second derivative CENTERED DERIVATIVE \n",
    "            \n",
    "            first_ht=[]   \n",
    "            first_derivative=[]\n",
    "            second_ht=[]  \n",
    "            second_derivative=[]\n",
    "            \n",
    "            data['HT']=data['HT'].astype(float)\n",
    "            data['SPD']=data['SPD'].astype(float)\n",
    "            \n",
    "            for k in np.arange(0,len(data['HT'])-1):\n",
    "                first_ht.append(data['HT'][k])\n",
    "                derivative= ( (data['SPD'][k+1]-data['SPD'][k])/(data['HT'][k+1]-data['HT'][k]) )\n",
    "                first_derivative.append(derivative)\n",
    "            \n",
    "            for k in np.arange(1,len(data['HT'])-1):\n",
    "                second_ht.append(data['HT'][k])\n",
    "                \n",
    "                derivative= ( (data['SPD'][k+1]-2*data['SPD'][k]+data['SPD'][k-1]) / ((data['HT'][k+1]-data['HT'][k])**2) )\n",
    "                second_derivative.append(derivative)\n",
    "            \n",
    "            \n",
    "            d = {'HT_first': first_ht, 'SPD_first': first_derivative}\n",
    "            FIRST_DERIVATIVE_TABLE = pd.DataFrame(data=d)\n",
    "            \n",
    "            dd = {'HT_second': second_ht, 'SPD_second': second_derivative}\n",
    "            SECOND_DERIVATIVE_TABLE = pd.DataFrame(data=dd)\n",
    "            \n",
    "            #print(FIRST_DERIVATIVE_TABLE)\n",
    "            #print(SECOND_DERIVATIVE_TABLE)\n",
    "            \n",
    "            \n",
    "            list_of_data_per_day.append(data)\n",
    "            list_of_first_derivative_per_day.append(FIRST_DERIVATIVE_TABLE)\n",
    "            list_of_second_derivative_per_day.append(SECOND_DERIVATIVE_TABLE)\n",
    "\n",
    "        TOTALDATA.append( [list_of_data_per_day, list_of_hours, list_of_first_derivative_per_day,list_of_second_derivative_per_day ] )\n",
    " \n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    datahistograms1=[] \n",
    "    datahistograms1st1=[] \n",
    "    datahistograms2nd1=[] \n",
    "    \n",
    "    datahistograms2=[] \n",
    "    datahistograms1st2=[] \n",
    "    datahistograms2nd2=[] \n",
    "    \n",
    "    datahistograms3=[] \n",
    "    datahistograms1st3=[] \n",
    "    datahistograms2nd3=[] \n",
    "    \n",
    "    datahistograms4=[] \n",
    "    datahistograms1st4=[] \n",
    "    datahistograms2nd4=[] \n",
    "    \n",
    "    \n",
    "    #Iteration for 4 time intervals\n",
    "    \n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=0 and TOTALDATA[k][1][l].hour<6: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms1.append( [ [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z])] , TOTALDATA[k][1][l] ])\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st1.append( [ [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z])] , TOTALDATA[k][1][l]  ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd1.append( [ [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z])], TOTALDATA[k][1][l]  ]  )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=6 and TOTALDATA[k][1][l].hour<12: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms2.append( [ [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z])] , TOTALDATA[k][1][l] ]) \n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st2.append( [ [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ]  , TOTALDATA[k][1][l]  ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd2.append( [ [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z])], TOTALDATA[k][1][l]  ]  )  \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=12 and TOTALDATA[k][1][l].hour<18: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms3.append(  [ [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z])], TOTALDATA[k][1][l] ]) \n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st3.append(  [ [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ]  , TOTALDATA[k][1][l]  ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd3.append( [ [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z])] , TOTALDATA[k][1][l]  ] )    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    for k in np.arange(0,len(TOTALDATA)): #for each day    \n",
    "        for l in np.arange(0,len(TOTALDATA[k][0])): #for each hour\n",
    "            if TOTALDATA[k][1][l].hour>=18:    #and TOTALDATA[k][1][l].hour<=hora2: #if we are in the right time interval\n",
    "                for z in np.arange(0,len(TOTALDATA[k][0][l]['HT'])):\n",
    "                    datahistograms4.append( [ [float(TOTALDATA[k][0][l]['HT'][z]), float(TOTALDATA[k][0][l]['SPD'][z])], TOTALDATA[k][1][l] ])  \n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][2][l]['HT_first'])):\n",
    "                    datahistograms1st4.append( [ [float(TOTALDATA[k][2][l]['HT_first'][z]), float(TOTALDATA[k][2][l]['SPD_first'][z]) ] , TOTALDATA[k][1][l]  ] )\n",
    "                \n",
    "                for z in np.arange(0,len(TOTALDATA[k][3][l]['HT_second'])):\n",
    "                    datahistograms2nd4.append( [ [float(TOTALDATA[k][3][l]['HT_second'][z]), float(TOTALDATA[k][3][l]['SPD_second'][z])] , TOTALDATA[k][1][l]  ] )    \n",
    "   \n",
    "\n",
    "    #ITERATIONS FOR EVERY MONTH \n",
    "    \n",
    "    histogram1month1=[] \n",
    "    histogram1month2=[] \n",
    "    histogram1month3=[] \n",
    "    histogram1month4=[] \n",
    "    histogram1month5=[] \n",
    "    histogram1month6=[] \n",
    "    histogram1month7=[] \n",
    "    histogram1month8=[] \n",
    "    histogram1month9=[] \n",
    "    histogram1month10=[] \n",
    "    histogram1month11=[] \n",
    "    histogram1month12=[] \n",
    "    \n",
    "    histogram1stmonth1=[] \n",
    "    histogram1stmonth2=[] \n",
    "    histogram1stmonth3=[] \n",
    "    histogram1stmonth4=[] \n",
    "    histogram1stmonth5=[] \n",
    "    histogram1stmonth6=[] \n",
    "    histogram1stmonth7=[] \n",
    "    histogram1stmonth8=[] \n",
    "    histogram1stmonth9=[] \n",
    "    histogram1stmonth10=[] \n",
    "    histogram1stmonth11=[] \n",
    "    histogram1stmonth12=[] \n",
    "    \n",
    "    \n",
    "    for k in np.arange(0,len(datahistograms1)):   \n",
    "        if datahistograms1[k][1].month==1:\n",
    "            histogram1month1.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==2:\n",
    "            histogram1month2.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==3:\n",
    "            histogram1month3.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==4:\n",
    "            histogram1month4.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==5:\n",
    "            histogram1month5.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==6:\n",
    "            histogram1month6.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==7:\n",
    "            histogram1month7.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==8:\n",
    "            histogram1month8.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==9:\n",
    "            histogram1month9.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==10:\n",
    "            histogram1month10.append(datahistograms1[k])    \n",
    "        if datahistograms1[k][1].month==11:\n",
    "            histogram1month11.append(datahistograms1[k])\n",
    "        if datahistograms1[k][1].month==12:\n",
    "            histogram1month12.append(datahistograms1[k])\n",
    "       \n",
    "    for k in np.arange(0,len(datahistograms1st1)):   \n",
    "        if datahistograms1st1[k][1].month==1:\n",
    "            histogram1stmonth1.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==2:\n",
    "            histogram1stmonth2.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==3:\n",
    "            histogram1stmonth3.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==4:\n",
    "            histogram1stmonth4.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==5:\n",
    "            histogram1stmonth5.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==6:\n",
    "            histogram1stmonth6.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==7:\n",
    "            histogram1stmonth7.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==8:\n",
    "            histogram1stmonth8.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==9:\n",
    "            histogram1stmonth9.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==10:\n",
    "            histogram1stmonth10.append(datahistograms1st1[k])    \n",
    "        if datahistograms1st1[k][1].month==11:\n",
    "            histogram1stmonth11.append(datahistograms1st1[k])\n",
    "        if datahistograms1st1[k][1].month==12:\n",
    "            histogram1stmonth12.append(datahistograms1st1[k])\n",
    "            \n",
    "            \n",
    "    histogram2month1=[] \n",
    "    histogram2month2=[] \n",
    "    histogram2month3=[] \n",
    "    histogram2month4=[] \n",
    "    histogram2month5=[] \n",
    "    histogram2month6=[] \n",
    "    histogram2month7=[] \n",
    "    histogram2month8=[] \n",
    "    histogram2month9=[] \n",
    "    histogram2month10=[] \n",
    "    histogram2month11=[] \n",
    "    histogram2month12=[]\n",
    "     \n",
    "    histogram1st2month1=[] \n",
    "    histogram1st2month2=[] \n",
    "    histogram1st2month3=[] \n",
    "    histogram1st2month4=[] \n",
    "    histogram1st2month5=[] \n",
    "    histogram1st2month6=[] \n",
    "    histogram1st2month7=[] \n",
    "    histogram1st2month8=[] \n",
    "    histogram1st2month9=[] \n",
    "    histogram1st2month10=[] \n",
    "    histogram1st2month11=[] \n",
    "    histogram1st2month12=[] \n",
    "    for k in np.arange(0,len(datahistograms2)):   \n",
    "        if datahistograms2[k][1].month==1:\n",
    "            histogram2month1.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==2:\n",
    "            histogram2month2.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==3:\n",
    "            histogram2month3.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==4:\n",
    "            histogram2month4.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==5:\n",
    "            histogram2month5.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==6:\n",
    "            histogram2month6.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==7:\n",
    "            histogram2month7.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==8:\n",
    "            histogram2month8.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==9:\n",
    "            histogram2month9.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==10:\n",
    "            histogram2month10.append(datahistograms2[k])    \n",
    "        if datahistograms2[k][1].month==11:\n",
    "            histogram2month11.append(datahistograms2[k])\n",
    "        if datahistograms2[k][1].month==12:\n",
    "            histogram2month12.append(datahistograms2[k])\n",
    "       \n",
    "    for k in np.arange(0,len(datahistograms1st2)):   \n",
    "        if datahistograms1st2[k][1].month==1:\n",
    "            histogram1st2month1.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==2:\n",
    "            histogram1st2month2.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==3:\n",
    "            histogram1st2month3.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==4:\n",
    "            histogram1st2month4.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==5:\n",
    "            histogram1st2month5.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==6:\n",
    "            histogram1st2month6.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==7:\n",
    "            histogram1st2month7.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==8:\n",
    "            histogram1st2month8.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==9:\n",
    "            histogram1st2month9.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==10:\n",
    "            histogram1st2month10.append(datahistograms1st2[k])    \n",
    "        if datahistograms1st2[k][1].month==11:\n",
    "            histogram1st2month11.append(datahistograms1st2[k])\n",
    "        if datahistograms1st2[k][1].month==12:\n",
    "            histogram1st2month12.append(datahistograms1st2[k])  \n",
    "    \n",
    "    \n",
    "    histogram3month1=[] \n",
    "    histogram3month2=[] \n",
    "    histogram3month3=[] \n",
    "    histogram3month4=[] \n",
    "    histogram3month5=[] \n",
    "    histogram3month6=[] \n",
    "    histogram3month7=[] \n",
    "    histogram3month8=[] \n",
    "    histogram3month9=[] \n",
    "    histogram3month10=[] \n",
    "    histogram3month11=[] \n",
    "    histogram3month12=[]\n",
    "     \n",
    "    histogram1st3month1=[] \n",
    "    histogram1st3month2=[] \n",
    "    histogram1st3month3=[] \n",
    "    histogram1st3month4=[] \n",
    "    histogram1st3month5=[] \n",
    "    histogram1st3month6=[] \n",
    "    histogram1st3month7=[] \n",
    "    histogram1st3month8=[] \n",
    "    histogram1st3month9=[] \n",
    "    histogram1st3month10=[] \n",
    "    histogram1st3month11=[] \n",
    "    histogram1st3month12=[] \n",
    "    for k in np.arange(0,len(datahistograms3)):   \n",
    "        if datahistograms3[k][1].month==1:\n",
    "            histogram3month1.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==2:\n",
    "            histogram3month2.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==3:\n",
    "            histogram3month3.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==4:\n",
    "            histogram3month4.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==5:\n",
    "            histogram3month5.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==6:\n",
    "            histogram3month6.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==7:\n",
    "            histogram3month7.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==8:\n",
    "            histogram3month8.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==9:\n",
    "            histogram3month9.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==10:\n",
    "            histogram3month10.append(datahistograms3[k])    \n",
    "        if datahistograms3[k][1].month==11:\n",
    "            histogram3month11.append(datahistograms3[k])\n",
    "        if datahistograms3[k][1].month==12:\n",
    "            histogram3month12.append(datahistograms3[k])\n",
    "       \n",
    "    for k in np.arange(0,len(datahistograms1st3)):   \n",
    "        if datahistograms1st3[k][1].month==1:\n",
    "            histogram1st3month1.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==2:\n",
    "            histogram1st3month2.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==3:\n",
    "            histogram1st3month3.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==4:\n",
    "            histogram1st3month4.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==5:\n",
    "            histogram1st3month5.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==6:\n",
    "            histogram1st3month6.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==7:\n",
    "            histogram1st3month7.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==8:\n",
    "            histogram1st3month8.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==9:\n",
    "            histogram1st3month9.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==10:\n",
    "            histogram1st3month10.append(datahistograms1st3[k])    \n",
    "        if datahistograms1st3[k][1].month==11:\n",
    "            histogram1st3month11.append(datahistograms1st3[k])\n",
    "        if datahistograms1st3[k][1].month==12:\n",
    "            histogram1st3month12.append(datahistograms1st3[k])  \n",
    "    \n",
    "    histogram4month1=[] \n",
    "    histogram4month2=[] \n",
    "    histogram4month3=[] \n",
    "    histogram4month4=[] \n",
    "    histogram4month5=[] \n",
    "    histogram4month6=[] \n",
    "    histogram4month7=[] \n",
    "    histogram4month8=[] \n",
    "    histogram4month9=[] \n",
    "    histogram4month10=[] \n",
    "    histogram4month11=[] \n",
    "    histogram4month12=[]\n",
    "     \n",
    "    histogram1st4month1=[] \n",
    "    histogram1st4month2=[] \n",
    "    histogram1st4month3=[] \n",
    "    histogram1st4month4=[] \n",
    "    histogram1st4month5=[] \n",
    "    histogram1st4month6=[] \n",
    "    histogram1st4month7=[] \n",
    "    histogram1st4month8=[] \n",
    "    histogram1st4month9=[] \n",
    "    histogram1st4month10=[] \n",
    "    histogram1st4month11=[] \n",
    "    histogram1st4month12=[] \n",
    "    for k in np.arange(0,len(datahistograms4)):   \n",
    "        if datahistograms4[k][1].month==1:\n",
    "            histogram4month1.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==2:\n",
    "            histogram4month2.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==3:\n",
    "            histogram4month3.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==4:\n",
    "            histogram4month4.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==5:\n",
    "            histogram4month5.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==6:\n",
    "            histogram4month6.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==7:\n",
    "            histogram4month7.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==8:\n",
    "            histogram4month8.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==9:\n",
    "            histogram4month9.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==10:\n",
    "            histogram4month10.append(datahistograms4[k])    \n",
    "        if datahistograms4[k][1].month==11:\n",
    "            histogram4month11.append(datahistograms4[k])\n",
    "        if datahistograms4[k][1].month==12:\n",
    "            histogram4month12.append(datahistograms4[k])\n",
    "       \n",
    "    for k in np.arange(0,len(datahistograms1st4)):   \n",
    "        if datahistograms1st4[k][1].month==1:\n",
    "            histogram1st4month1.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==2:\n",
    "            histogram1st4month2.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==3:\n",
    "            histogram1st4month3.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==4:\n",
    "            histogram1st4month4.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==5:\n",
    "            histogram1st4month5.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==6:\n",
    "            histogram1st4month6.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==7:\n",
    "            histogram1st4month7.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==8:\n",
    "            histogram1st4month8.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==9:\n",
    "            histogram1st4month9.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==10:\n",
    "            histogram1st4month10.append(datahistograms1st4[k])    \n",
    "        if datahistograms1st4[k][1].month==11:\n",
    "            histogram1st4month11.append(datahistograms1st4[k])\n",
    "        if datahistograms1st4[k][1].month==12:\n",
    "            histogram1st4month12.append(datahistograms1st4[k])  \n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    def MeanAndStd(datahistograms,km1,km2):\n",
    "        data=[]\n",
    "        for i in np.arange(0,len(datahistograms)):\n",
    "            if km1<=datahistograms[i][0][0]<=km2:\n",
    "                data.append(datahistograms[i][0][1]) \n",
    "        mean=np.nanmean(data)\n",
    "        std=np.nanstd(data)\n",
    "\n",
    "        return  mean, std\n",
    "    \n",
    "    def ITERATION(datahistograms,datahistograms1st):\n",
    "        a1=[[0.180,0.220], [0.280,0.320], [0.380,0.420],[0.480,0.520],[0.580,0.620],[0.680,0.720],[0.780,0.820],[0.880,0.920],[0.980,1.020],[1.080,1.120],[1.180,1.220],[1.280,1.320],[1.380,1.420]]\n",
    "\n",
    "        meanlist=[]\n",
    "        stdlist=[]\n",
    "        \n",
    "        meanlist1st=[]\n",
    "        stdlist1st=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(0,len(a1)):\n",
    "            mean,std= MeanAndStd(datahistograms,a1[i][0],a1[i][1])\n",
    "            meanlist.append(mean)\n",
    "            stdlist.append(std)\n",
    "            \n",
    "        for i in np.arange(0,len(a1)-1):\n",
    "            mean,std= MeanAndStd(datahistograms1st,a1[i][0],a1[i][1])\n",
    "            meanlist1st.append(mean)\n",
    "            stdlist1st.append(std)\n",
    "              \n",
    "            \n",
    "\n",
    "        return meanlist, stdlist, a1,  meanlist1st,stdlist1st\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    datapermonth1=[ histogram1month1,histogram1month2,histogram1month3,histogram1month4,histogram1month5,histogram1month6,histogram1month7,histogram1month8,histogram1month9,histogram1month10,histogram1month11,histogram1month12] \n",
    "    datapermonth1st1=[ histogram1stmonth1,histogram1stmonth2,histogram1stmonth3,histogram1stmonth4,histogram1stmonth5,histogram1stmonth6,histogram1stmonth7,histogram1stmonth8,histogram1stmonth9,histogram1stmonth10,histogram1stmonth11,histogram1stmonth12] \n",
    "    \n",
    "    datapermonth2=[ histogram2month1,histogram2month2,histogram2month3,histogram2month4,histogram2month5,histogram2month6,histogram2month7,histogram2month8,histogram2month9,histogram2month10,histogram2month11,histogram2month12] \n",
    "    datapermonth1st2=[ histogram1st2month1,histogram1st2month2,histogram1st2month3,histogram1st2month4,histogram1st2month5,histogram1st2month6,histogram1st2month7,histogram1st2month8,histogram1st2month9,histogram1st2month10,histogram1st2month11,histogram1st2month12] \n",
    "    \n",
    "    datapermonth3=[ histogram3month1,histogram3month2,histogram3month3,histogram3month4,histogram3month5,histogram3month6,histogram3month7,histogram3month8,histogram3month9,histogram3month10,histogram3month11,histogram3month12] \n",
    "    datapermonth1st3=[ histogram1st3month1,histogram1st3month2,histogram1st3month3,histogram1st3month4,histogram1st3month5,histogram1st3month6,histogram1st3month7,histogram1st3month8,histogram1st3month9,histogram1st3month10,histogram1st3month11,histogram1st3month12] \n",
    "    \n",
    "    datapermonth4=[ histogram4month1,histogram4month2,histogram4month3,histogram4month4,histogram4month5,histogram4month6,histogram4month7,histogram4month8,histogram4month9,histogram4month10,histogram4month11,histogram4month12] \n",
    "    datapermonth1st4=[ histogram1st4month1,histogram1st4month2,histogram1st4month3,histogram1st4month4,histogram1st4month5,histogram1st4month6,histogram1st4month7,histogram1st4month8,histogram1st4month9,histogram1st4month10,histogram1st4month11,histogram1st4month12] \n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------------------------\n",
    "    listfofmeans1=[]\n",
    "    listfodeviations1=[]\n",
    "    listfofmeans1st1=[]\n",
    "    listfofdeviations1st1=[]\n",
    "    for k in np.arange(0,len(datapermonth1)):\n",
    "        a1=datapermonth1[k]\n",
    "        a2=datapermonth1st1[k]\n",
    "        means1,deviations1,kilometers1 , means1st1,deviations1st1 = ITERATION(a1, a2)\n",
    "        listfofmeans1.append(means1)\n",
    "        listfodeviations1.append(deviations1)\n",
    "        listfofmeans1st1.append(means1st1)\n",
    "        listfofdeviations1st1.append(deviations1st1)\n",
    "    \n",
    "    listfofmeans2=[]\n",
    "    listfodeviations2=[]\n",
    "    listfofmeans1st2=[]\n",
    "    listfofdeviations1st2=[]\n",
    "    for k in np.arange(0,len(datapermonth2)):\n",
    "        a1=datapermonth2[k]\n",
    "        a2=datapermonth1st2[k]\n",
    "        means1,deviations1,kilometers1 , means1st1,deviations1st1 = ITERATION(a1, a2)\n",
    "        listfofmeans2.append(means1)\n",
    "        listfodeviations2.append(deviations1)\n",
    "        listfofmeans1st2.append(means1st1)\n",
    "        listfofdeviations1st2.append(deviations1st1)\n",
    "        \n",
    "    listfofmeans3=[]\n",
    "    listfodeviations3=[]\n",
    "    listfofmeans1st3=[]\n",
    "    listfofdeviations1st3=[]\n",
    "    for k in np.arange(0,len(datapermonth3)):\n",
    "        a1=datapermonth3[k]\n",
    "        a2=datapermonth1st3[k]\n",
    "        means1,deviations1,kilometers1 , means1st1,deviations1st1 = ITERATION(a1, a2)\n",
    "        listfofmeans3.append(means1)\n",
    "        listfodeviations3.append(deviations1)\n",
    "        listfofmeans1st3.append(means1st1)\n",
    "        listfofdeviations1st3.append(deviations1st1)    \n",
    "    \n",
    "    listfofmeans4=[]\n",
    "    listfodeviations4=[]\n",
    "    listfofmeans1st4=[]\n",
    "    listfofdeviations1st4=[]\n",
    "    for k in np.arange(0,len(datapermonth4)):\n",
    "        a1=datapermonth4[k]\n",
    "        a2=datapermonth1st4[k]\n",
    "        means1,deviations1,kilometers1 , means1st1,deviations1st1 = ITERATION(a1, a2)\n",
    "        listfofmeans4.append(means1)\n",
    "        listfodeviations4.append(deviations1)\n",
    "        listfofmeans1st4.append(means1st1)\n",
    "        listfofdeviations1st4.append(deviations1st1)    \n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    dataplotkm1=[]\n",
    "    dataplotkm2=[]\n",
    "    dataplotkm3=[]\n",
    "    dataplotkm4=[]\n",
    "    dataplotkm5=[]\n",
    "    dataplotkm6=[]\n",
    "    dataplotkm7=[]\n",
    "    dataplotkm8=[]\n",
    "    dataplotkm9=[]\n",
    "    dataplotkm10=[]\n",
    "    dataplotkm11=[]\n",
    "    dataplotkm12=[]\n",
    "    dataplotkm13=[]\n",
    "    \n",
    "    dataplotkm1error=[]\n",
    "    dataplotkm2error=[]\n",
    "    dataplotkm3error=[]\n",
    "    dataplotkm4error=[]\n",
    "    dataplotkm5error=[]\n",
    "    dataplotkm6error=[]\n",
    "    dataplotkm7error=[]\n",
    "    dataplotkm8error=[]\n",
    "    dataplotkm9error=[]\n",
    "    dataplotkm10error=[]\n",
    "    dataplotkm11error=[]\n",
    "    dataplotkm12error=[]\n",
    "    dataplotkm13error=[]\n",
    "    \n",
    "    \n",
    "    dataplot2km1=[]\n",
    "    dataplot2km2=[]\n",
    "    dataplot2km3=[]\n",
    "    dataplot2km4=[]\n",
    "    dataplot2km5=[]\n",
    "    dataplot2km6=[]\n",
    "    dataplot2km7=[]\n",
    "    dataplot2km8=[]\n",
    "    dataplot2km9=[]\n",
    "    dataplot2km10=[]\n",
    "    dataplot2km11=[]\n",
    "    dataplot2km12=[]\n",
    "    dataplot2km13=[]\n",
    "    \n",
    "    dataplot2km1error=[]\n",
    "    dataplot2km2error=[]\n",
    "    dataplot2km3error=[]\n",
    "    dataplot2km4error=[]\n",
    "    dataplot2km5error=[]\n",
    "    dataplot2km6error=[]\n",
    "    dataplot2km7error=[]\n",
    "    dataplot2km8error=[]\n",
    "    dataplot2km9error=[]\n",
    "    dataplot2km10error=[]\n",
    "    dataplot2km11error=[]\n",
    "    dataplot2km12error=[]\n",
    "    dataplot2km13error=[]\n",
    "    \n",
    "    \n",
    "    dataplot3km1=[]\n",
    "    dataplot3km2=[]\n",
    "    dataplot3km3=[]\n",
    "    dataplot3km4=[]\n",
    "    dataplot3km5=[]\n",
    "    dataplot3km6=[]\n",
    "    dataplot3km7=[]\n",
    "    dataplot3km8=[]\n",
    "    dataplot3km9=[]\n",
    "    dataplot3km10=[]\n",
    "    dataplot3km11=[]\n",
    "    dataplot3km12=[]\n",
    "    dataplot3km13=[]\n",
    "    \n",
    "    dataplot3km1error=[]\n",
    "    dataplot3km2error=[]\n",
    "    dataplot3km3error=[]\n",
    "    dataplot3km4error=[]\n",
    "    dataplot3km5error=[]\n",
    "    dataplot3km6error=[]\n",
    "    dataplot3km7error=[]\n",
    "    dataplot3km8error=[]\n",
    "    dataplot3km9error=[]\n",
    "    dataplot3km10error=[]\n",
    "    dataplot3km11error=[]\n",
    "    dataplot3km12error=[]\n",
    "    dataplot3km13error=[]\n",
    "    \n",
    "    \n",
    "    dataplot4km1=[]\n",
    "    dataplot4km2=[]\n",
    "    dataplot4km3=[]\n",
    "    dataplot4km4=[]\n",
    "    dataplot4km5=[]\n",
    "    dataplot4km6=[]\n",
    "    dataplot4km7=[]\n",
    "    dataplot4km8=[]\n",
    "    dataplot4km9=[]\n",
    "    dataplot4km10=[]\n",
    "    dataplot4km11=[]\n",
    "    dataplot4km12=[]\n",
    "    dataplot4km13=[]\n",
    "    \n",
    "    dataplot4km1error=[]\n",
    "    dataplot4km2error=[]\n",
    "    dataplot4km3error=[]\n",
    "    dataplot4km4error=[]\n",
    "    dataplot4km5error=[]\n",
    "    dataplot4km6error=[]\n",
    "    dataplot4km7error=[]\n",
    "    dataplot4km8error=[]\n",
    "    dataplot4km9error=[]\n",
    "    dataplot4km10error=[]\n",
    "    dataplot4km11error=[]\n",
    "    dataplot4km12error=[]\n",
    "    dataplot4km13error=[]\n",
    "    \n",
    "    for k in np.arange(0,len(listfofmeans1)):\n",
    "        dataplotkm1.append(listfofmeans1[k][0])\n",
    "        dataplotkm2.append(listfofmeans1[k][1])\n",
    "        dataplotkm3.append(listfofmeans1[k][2])\n",
    "        dataplotkm4.append(listfofmeans1[k][3])\n",
    "        dataplotkm5.append(listfofmeans1[k][4])\n",
    "        dataplotkm6.append(listfofmeans1[k][5])\n",
    "        dataplotkm7.append(listfofmeans1[k][6])\n",
    "        dataplotkm8.append(listfofmeans1[k][7])\n",
    "        dataplotkm9.append(listfofmeans1[k][8])\n",
    "        dataplotkm10.append(listfofmeans1[k][9])\n",
    "        dataplotkm11.append(listfofmeans1[k][10])\n",
    "        dataplotkm12.append(listfofmeans1[k][11])\n",
    "        dataplotkm13.append(listfofmeans1[k][12])\n",
    "        \n",
    "        dataplotkm1error.append(listfodeviations1[k][0])\n",
    "        dataplotkm2error.append(listfodeviations1[k][1])\n",
    "        dataplotkm3error.append(listfodeviations1[k][2])\n",
    "        dataplotkm4error.append(listfodeviations1[k][3])\n",
    "        dataplotkm5error.append(listfodeviations1[k][4])\n",
    "        dataplotkm6error.append(listfodeviations1[k][5])\n",
    "        dataplotkm7error.append(listfodeviations1[k][6])\n",
    "        dataplotkm8error.append(listfodeviations1[k][7])\n",
    "        dataplotkm9error.append(listfodeviations1[k][8])\n",
    "        dataplotkm10error.append(listfodeviations1[k][9])\n",
    "        dataplotkm11error.append(listfodeviations1[k][10])\n",
    "        dataplotkm12error.append(listfodeviations1[k][11])\n",
    "        dataplotkm13error.append(listfodeviations1[k][12])\n",
    "        \n",
    "        dataplot2km1.append(listfofmeans2[k][0])\n",
    "        dataplot2km2.append(listfofmeans2[k][1])\n",
    "        dataplot2km3.append(listfofmeans2[k][2])\n",
    "        dataplot2km4.append(listfofmeans2[k][3])\n",
    "        dataplot2km5.append(listfofmeans2[k][4])\n",
    "        dataplot2km6.append(listfofmeans2[k][5])\n",
    "        dataplot2km7.append(listfofmeans2[k][6])\n",
    "        dataplot2km8.append(listfofmeans2[k][7])\n",
    "        dataplot2km9.append(listfofmeans2[k][8])\n",
    "        dataplot2km10.append(listfofmeans2[k][9])\n",
    "        dataplot2km11.append(listfofmeans2[k][10])\n",
    "        dataplot2km12.append(listfofmeans2[k][11])\n",
    "        dataplot2km13.append(listfofmeans2[k][12])\n",
    "        \n",
    "        dataplot2km1error.append(listfodeviations2[k][0])\n",
    "        dataplot2km2error.append(listfodeviations2[k][1])\n",
    "        dataplot2km3error.append(listfodeviations2[k][2])\n",
    "        dataplot2km4error.append(listfodeviations2[k][3])\n",
    "        dataplot2km5error.append(listfodeviations2[k][4])\n",
    "        dataplot2km6error.append(listfodeviations2[k][5])\n",
    "        dataplot2km7error.append(listfodeviations2[k][6])\n",
    "        dataplot2km8error.append(listfodeviations2[k][7])\n",
    "        dataplot2km9error.append(listfodeviations2[k][8])\n",
    "        dataplot2km10error.append(listfodeviations2[k][9])\n",
    "        dataplot2km11error.append(listfodeviations2[k][10])\n",
    "        dataplot2km12error.append(listfodeviations2[k][11])\n",
    "        dataplot2km13error.append(listfodeviations2[k][12])\n",
    "        \n",
    "        \n",
    "        dataplot3km1.append(listfofmeans3[k][0])\n",
    "        dataplot3km2.append(listfofmeans3[k][1])\n",
    "        dataplot3km3.append(listfofmeans3[k][2])\n",
    "        dataplot3km4.append(listfofmeans3[k][3])\n",
    "        dataplot3km5.append(listfofmeans3[k][4])\n",
    "        dataplot3km6.append(listfofmeans3[k][5])\n",
    "        dataplot3km7.append(listfofmeans3[k][6])\n",
    "        dataplot3km8.append(listfofmeans3[k][7])\n",
    "        dataplot3km9.append(listfofmeans3[k][8])\n",
    "        dataplot3km10.append(listfofmeans3[k][9])\n",
    "        dataplot3km11.append(listfofmeans3[k][10])\n",
    "        dataplot3km12.append(listfofmeans3[k][11])\n",
    "        dataplot3km13.append(listfofmeans3[k][12])\n",
    "        \n",
    "        dataplot3km1error.append(listfodeviations3[k][0])\n",
    "        dataplot3km2error.append(listfodeviations3[k][1])\n",
    "        dataplot3km3error.append(listfodeviations3[k][2])\n",
    "        dataplot3km4error.append(listfodeviations3[k][3])\n",
    "        dataplot3km5error.append(listfodeviations3[k][4])\n",
    "        dataplot3km6error.append(listfodeviations3[k][5])\n",
    "        dataplot3km7error.append(listfodeviations3[k][6])\n",
    "        dataplot3km8error.append(listfodeviations3[k][7])\n",
    "        dataplot3km9error.append(listfodeviations3[k][8])\n",
    "        dataplot3km10error.append(listfodeviations3[k][9])\n",
    "        dataplot3km11error.append(listfodeviations3[k][10])\n",
    "        dataplot3km12error.append(listfodeviations3[k][11])\n",
    "        dataplot3km13error.append(listfodeviations3[k][12])\n",
    "        \n",
    "        \n",
    "        dataplot4km1.append(listfofmeans4[k][0])\n",
    "        dataplot4km2.append(listfofmeans4[k][1])\n",
    "        dataplot4km3.append(listfofmeans4[k][2])\n",
    "        dataplot4km4.append(listfofmeans4[k][3])\n",
    "        dataplot4km5.append(listfofmeans4[k][4])\n",
    "        dataplot4km6.append(listfofmeans4[k][5])\n",
    "        dataplot4km7.append(listfofmeans4[k][6])\n",
    "        dataplot4km8.append(listfofmeans4[k][7])\n",
    "        dataplot4km9.append(listfofmeans4[k][8])\n",
    "        dataplot4km10.append(listfofmeans4[k][9])\n",
    "        dataplot4km11.append(listfofmeans4[k][10])\n",
    "        dataplot4km12.append(listfofmeans4[k][11])\n",
    "        dataplot4km13.append(listfofmeans4[k][12])\n",
    "        \n",
    "        dataplot4km1error.append(listfodeviations4[k][0])\n",
    "        dataplot4km2error.append(listfodeviations4[k][1])\n",
    "        dataplot4km3error.append(listfodeviations4[k][2])\n",
    "        dataplot4km4error.append(listfodeviations4[k][3])\n",
    "        dataplot4km5error.append(listfodeviations4[k][4])\n",
    "        dataplot4km6error.append(listfodeviations4[k][5])\n",
    "        dataplot4km7error.append(listfodeviations4[k][6])\n",
    "        dataplot4km8error.append(listfodeviations4[k][7])\n",
    "        dataplot4km9error.append(listfodeviations4[k][8])\n",
    "        dataplot4km10error.append(listfodeviations4[k][9])\n",
    "        dataplot4km11error.append(listfodeviations4[k][10])\n",
    "        dataplot4km12error.append(listfodeviations4[k][11])\n",
    "        dataplot4km13error.append(listfodeviations4[k][12])\n",
    "            \n",
    "               \n",
    "      \n",
    " \n",
    "               \n",
    "            \n",
    "    #PLOT\n",
    "    ylim1=0\n",
    "    ylim2=25\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    x=np.arange(0,12)\n",
    "    \n",
    "\n",
    "    ax7=plt.subplot(616)\n",
    "\n",
    "    figsize=(20,4) \n",
    "    plt.errorbar(x, dataplotkm6, yerr=dataplotkm6error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km6, yerr=dataplot2km6error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km6, yerr=dataplot3km6error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km6, yerr=dataplot4km6error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    #plt.text(0.01, 0.75, \"E)\", fontweight=\"bold\", fontsize=13 ,transform=ax7.transAxes)\n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax7.set_ylim([ylim1,ylim2])\n",
    "    #specify x-axis locations\n",
    "    x_ticks = x\n",
    "    #specify x-axis labels\n",
    "    x_labels = ['Enero', '','Marzo','','Mayo','','Julio','','Septiembre','','Noviembre',''] \n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels,fontsize=13)\n",
    "    #ax7.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "\n",
    "    plt.text(-0.06, 0.42, \"F)\", fontweight=\"bold\", fontsize=13 ,transform=ax7.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "    #FIRST PLOT\n",
    "\n",
    "    ax1=plt.subplot(611,sharex=ax7)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm1, yerr=dataplotkm1error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km1, yerr=dataplot2km1error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km1, yerr=dataplot3km1error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km1, yerr=dataplot4km1error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax1.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"A)\", fontweight=\"bold\", fontsize=13 ,transform=ax1.transAxes)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #SECOND PLOT\n",
    "    ax2=plt.subplot(612,sharex=ax7)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm2, yerr=dataplotkm2error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km2, yerr=dataplot2km2error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km2, yerr=dataplot3km2error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km2, yerr=dataplot4km2error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax2.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"B)\", fontweight=\"bold\", fontsize=13 ,transform=ax2.transAxes)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #THIRD PLOT\n",
    "    ax3=plt.subplot(613,sharex=ax7)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm3, yerr=dataplotkm3error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km3, yerr=dataplot2km3error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km3, yerr=dataplot3km3error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km3, yerr=dataplot4km3error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax3.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"C)\", fontweight=\"bold\", fontsize=13 ,transform=ax3.transAxes)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FOURTH PLOT\n",
    "    ax4=plt.subplot(614,sharex=ax7)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm4, yerr=dataplotkm4error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km4, yerr=dataplot2km4error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km4, yerr=dataplot3km4error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km4, yerr=dataplot4km4error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "\n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax4.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"D)\", fontweight=\"bold\", fontsize=13 ,transform=ax4.transAxes)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax5=plt.subplot(615,sharex=ax7)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm5, yerr=dataplotkm5error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km5, yerr=dataplot2km5error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km5, yerr=dataplot3km5error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km5, yerr=dataplot4km5error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"E)\", fontweight=\"bold\", fontsize=13 ,transform=ax5.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #PLOT\n",
    "    ylim1=0\n",
    "    ylim2=25\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    x=np.arange(0,12)\n",
    "    \n",
    "\n",
    "    ax12=plt.subplot(6,2,12)\n",
    "\n",
    "    figsize=(20,4) \n",
    "    plt.errorbar(x, dataplotkm12, yerr=dataplotkm12error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km12, yerr=dataplot2km12error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km12, yerr=dataplot3km12error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km12, yerr=dataplot4km12error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    #plt.text(0.01, 0.75, \"E)\", fontweight=\"bold\", fontsize=13 ,transform=ax7.transAxes)\n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax12.set_ylim([ylim1,ylim2])\n",
    "    #specify x-axis locations\n",
    "    x_ticks = x\n",
    "    #specify x-axis labels\n",
    "    x_labels = ['Enero', '','Marzo','','Mayo','','Julio','','Septiembre','','Noviembre',''] \n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels,fontsize=13)\n",
    "    #ax7.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "\n",
    "    plt.text(-0.06, 0.42, \"L)\", fontweight=\"bold\", fontsize=13 ,transform=ax12.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "    #FIRST PLOT\n",
    "\n",
    "    ax1=plt.subplot(6,2,1,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm1, yerr=dataplotkm1error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km1, yerr=dataplot2km1error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km1, yerr=dataplot3km1error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km1, yerr=dataplot4km1error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax1.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"A)\", fontweight=\"bold\", fontsize=13 ,transform=ax1.transAxes)\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #SECOND PLOT\n",
    "    ax2=plt.subplot(6,2,2,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm7, yerr=dataplotkm7error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km7, yerr=dataplot2km7error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km7, yerr=dataplot3km7error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km7, yerr=dataplot4km7error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax2.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"G)\", fontweight=\"bold\", fontsize=13 ,transform=ax2.transAxes)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #THIRD PLOT\n",
    "    ax3=plt.subplot(6,2,3,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm2, yerr=dataplotkm2error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km2, yerr=dataplot2km2error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km2, yerr=dataplot3km2error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km2, yerr=dataplot4km2error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "    \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax3.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"B)\", fontweight=\"bold\", fontsize=13 ,transform=ax3.transAxes)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FOURTH PLOT\n",
    "    ax4=plt.subplot(6,2,4,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm8, yerr=dataplotkm8error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km8, yerr=dataplot2km8error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km8, yerr=dataplot3km8error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km8, yerr=dataplot4km8error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "\n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax4.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"H)\", fontweight=\"bold\", fontsize=13 ,transform=ax4.transAxes)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "\n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax5=plt.subplot(6,2,5,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm3, yerr=dataplotkm3error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km3, yerr=dataplot2km3error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km3, yerr=dataplot3km3error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km3, yerr=dataplot4km3error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"C)\", fontweight=\"bold\", fontsize=13 ,transform=ax5.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax6=plt.subplot(6,2,6,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm9, yerr=dataplotkm9error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km9, yerr=dataplot2km9error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km9, yerr=dataplot3km9error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km9, yerr=dataplot4km9error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"I)\", fontweight=\"bold\", fontsize=13 ,transform=ax6.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax7=plt.subplot(6,2,7,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm4, yerr=dataplotkm4error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km4, yerr=dataplot2km4error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km4, yerr=dataplot3km4error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km4, yerr=dataplot4km4error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"D)\", fontweight=\"bold\", fontsize=13 ,transform=ax7.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax8=plt.subplot(6,2,8,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm10, yerr=dataplotkm10error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km10, yerr=dataplot2km10error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km10, yerr=dataplot3km10error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km10, yerr=dataplot4km10error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"J)\", fontweight=\"bold\", fontsize=13 ,transform=ax8.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax9=plt.subplot(6,2,9,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm5, yerr=dataplotkm5error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km5, yerr=dataplot2km5error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km5, yerr=dataplot3km5error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km5, yerr=dataplot4km5error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"E)\", fontweight=\"bold\", fontsize=13 ,transform=ax9.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax10=plt.subplot(6,2,10,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm11, yerr=dataplotkm11error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km11, yerr=dataplot2km11error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km11, yerr=dataplot3km11error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km11, yerr=dataplot4km11error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"K)\", fontweight=\"bold\", fontsize=13 ,transform=ax10.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "    #FIFTH PLOT\n",
    "    ax11=plt.subplot(6,2,11,sharex=ax12)\n",
    "\n",
    "    figsize=(20,4)\n",
    "    plt.errorbar(x, dataplotkm6, yerr=dataplotkm6error, linestyle='-', marker='o', capsize=5, label='00:00-06:00')\n",
    "    plt.errorbar(x, dataplot2km6, yerr=dataplot2km6error, linestyle='-', marker='o', capsize=5, label='06:00-12:00')\n",
    "    plt.errorbar(x, dataplot3km6, yerr=dataplot3km6error, linestyle='-', marker='o', capsize=5, label='12:00-18:00')\n",
    "    plt.errorbar(x, dataplot4km6, yerr=dataplot4km6error, linestyle='-', marker='o', capsize=5, label='18:00-24:00')\n",
    "   \n",
    "    plt.ylabel('V  (m/s) ',fontsize=13)\n",
    "    ax5.set_ylim([ylim1,ylim2])\n",
    "    plt.text(-0.06, 0.42, \"F)\", fontweight=\"bold\", fontsize=13 ,transform=ax11.transAxes)\n",
    "    plt.setp(ax5.get_xticklabels(), visible=False)\n",
    "    \n",
    "    #ax1.get_yaxis().set_label_coords(-0.05,0.5)\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d827bf-e630-483e-89a9-230629d8d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365']\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/001\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/002\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/003\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/004\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/005\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/006\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/007\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/008\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/009\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/010\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/011\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/012\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/013\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/014\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/015\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/016\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/017\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/018\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/019\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/020\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/021\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/022\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/024\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/025\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/026\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/027\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/028\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/029\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/030\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/031\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/032\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/033\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/034\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/035\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/036\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/037\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/038\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/039\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/040\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/041\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/042\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/043\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/044\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/045\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/046\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/047\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/048\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/049\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/050\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/051\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/052\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/053\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/054\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/055\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/056\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/057\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/058\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/059\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/060\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/061\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/062\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/063\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/064\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/065\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/066\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/067\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/068\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/069\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/070\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/071\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/072\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/073\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/074\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/075\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/076\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/077\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/078\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/079\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/080\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/081\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/082\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/083\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/084\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/085\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/086\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/087\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/088\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/089\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/090\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/091\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/092\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/093\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/094\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/095\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/096\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/097\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/098\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/099\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/100\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/101\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/102\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/103\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/104\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/105\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/106\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/107\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/108\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/109\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/110\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/111\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/112\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/113\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/114\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/115\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/116\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/117\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/118\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/119\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/120\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/121\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/122\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/123\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/124\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/126\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/127\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/128\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/129\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/130\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/131\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/132\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/133\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/134\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/135\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/136\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/137\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/138\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/139\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/140\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/141\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/142\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/143\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/144\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/145\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/146\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/147\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/148\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/149\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/150\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/151\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/152\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/153\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/154\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/155\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/156\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/157\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/158\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/159\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/160\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/161\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/162\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/163\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/164\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/165\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/166\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/167\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/168\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/169\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/170\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/171\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/172\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/173\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/174\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/175\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/176\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/177\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/178\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/179\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/180\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/181\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/182\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/183\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/184\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/185\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/186\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/187\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/188\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/189\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/190\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/191\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/192\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/193\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/194\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/195\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/196\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/197\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/198\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/199\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/200\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/201\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/202\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/203\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/204\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/205\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/206\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/207\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/208\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/209\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/210\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/211\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/212\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/213\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/214\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/215\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/216\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/217\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/218\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/219\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/220\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/221\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/222\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/223\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/224\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/225\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/226\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/228\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/229\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/230\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/231\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/232\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/233\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/234\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/235\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/236\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/237\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/238\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/239\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/240\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/241\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/242\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/243\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/244\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/245\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/246\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/247\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/248\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/249\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/250\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/251\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/252\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/253\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/254\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/255\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/256\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/257\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/258\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/259\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/260\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/261\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/262\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/263\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/264\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/265\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/266\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/267\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/268\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/269\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/270\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/271\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/272\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/273\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/274\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/275\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/276\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/277\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/278\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/279\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/280\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/281\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/282\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/283\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/284\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/285\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/286\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/287\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/288\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/289\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/290\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/291\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/292\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/293\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/294\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/295\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/296\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/297\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/298\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/299\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/300\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/301\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/302\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/303\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/304\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/305\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/316\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/317\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/318\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/319\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/320\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/321\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/322\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/323\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/324\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/325\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/326\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/327\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/328\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/329\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/330\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/331\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/332\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/333\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/334\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/335\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/336\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/337\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/338\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/340\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/341\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/342\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/343\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/344\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/345\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/346\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/347\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/348\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/349\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/350\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/351\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/352\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/353\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/354\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/355\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/356\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/357\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/358\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/359\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/360\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/361\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/362\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/363\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/364\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018/365\n"
     ]
    }
   ],
   "source": [
    "GlobalFunction3('C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efa8038-65a2-452b-954a-c2dec9d7739c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '321', '322', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365']\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/270\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/271\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/272\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/273\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/274\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/275\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/276\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/277\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/278\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/279\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/280\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/281\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/282\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/283\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/284\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/285\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/286\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/287\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/288\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/289\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/290\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/291\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/292\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/321\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/322\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/332\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/333\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/334\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/335\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/336\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/337\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/338\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/339\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/340\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/341\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/342\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/343\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/344\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/345\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/346\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/347\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/348\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/349\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/350\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/351\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/352\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/353\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/355\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/356\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/357\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/358\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/359\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/360\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/361\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/362\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/363\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/364\n",
      "C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4/365\n"
     ]
    }
   ],
   "source": [
    "#GlobalFunction('C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2019subsets/4',18,23)\n",
    "\n",
    "#GlobalFunction2('C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2019subsets/4')\n",
    "#GlobalFunction2('C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2018subsets/4')\n",
    "GlobalFunction2('C:/Users/enclab01/Desktop/THESIS/Datos Viento/WwWind/pts/2017subsets/4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
